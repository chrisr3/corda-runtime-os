{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b39d83d-c2c8-4798-97b8-895485e859f9",
   "metadata": {},
   "source": [
    "# Trace-Analysis tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c143a55-7c60-4f0d-838d-86b38a1c8f03",
   "metadata": {},
   "source": [
    "The following notebook can be used to retrieve trace spans pertaining to one particular trace, and present them in a spreadsheet that clearly illustrates the different hops in the system, duration of the spans and the lag between these spans. The functionality of the tool can be furthered depending on personal analytical needs. \n",
    "\n",
    "For the notebook to run without problems, ensure that the zipkin server connection has been established and that some sort of flow/s has been started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89996913-b63b-4fc7-a766-7a78c90a7cfd",
   "metadata": {},
   "source": [
    "First step is to run the cell below to have all the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0d7ee5f-02a8-4ee1-a13c-0ef47cb741ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL NECESSARY IMPORTS\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from operator import attrgetter\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import openpyxl\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.styles import Border, Side\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88700ace-4b8d-4cae-981f-5f547e067628",
   "metadata": {},
   "source": [
    "Firstly, we retrieve the paths of the files that started the flows, such that we know which flows are being executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5691f4b5-7eb5-4584-83a1-6e335dba4fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com.r3.corda.testing.smoketests.flow.RpcSmokeTestFlow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               flow tags\n",
       "0  com.r3.corda.testing.smoketests.flow.RpcSmokeTestFlow"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RETRIEVE ALL STARTED FLOWS\n",
    "\n",
    "# Get flow tags\n",
    "url = \"http://tempo:3200/api/search/tag/flow.class/values\"\n",
    "\n",
    "# Make the API call and get the JSON response\n",
    "response = requests.get(url)\n",
    "response_json = response.json()\n",
    "\n",
    "# Retrieve the flow tags\n",
    "flow_tags = response_json[\"tagValues\"]\n",
    "\n",
    "# Create a pandas DataFrame from the list of flow tags\n",
    "df = pd.DataFrame({\"flow tags\": flow_tags})\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the DataFrame\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49f2472b-2042-483a-9910-4b4aed7cb121",
   "metadata": {},
   "source": [
    "Now, from the list above, we can select the flow we are interested in and retrieve the trace ID and respective request ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c8c48088-6c2d-4fca-9d9b-4f486551c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace ID</th>\n",
       "      <th>request ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64de181b901a77350e05bdb5e114eddb</td>\n",
       "      <td>1f491b89-885b-4d0f-82ca-16f5c745d0b1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           trace ID                            request ID\n",
       "0  64de181b901a77350e05bdb5e114eddb  1f491b89-885b-4d0f-82ca-16f5c745d0b1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ILLUSTRATE TRACE ID and REQUEST_ID MATCHING FOR A GIVEN FLOW\n",
    "\n",
    "# Change this string to match the flow you are interested in.\n",
    "flow = \"com.r3.corda.testing.smoketests.flow.RpcSmokeTestFlow\"\n",
    "\n",
    "# Define the URL and parameters\n",
    "url = \"http://tempo:3200/api/search\"\n",
    "params = {\n",
    "    \"tags\": \"flow.class=com.r3.corda.testing.smoketests.flow.RpcSmokeTestFlow\"\n",
    "}\n",
    "\n",
    "# FUNCTION TO RETRIEVE THE REQUEST ID FOR A GIVEN TRACE\n",
    "def get_request_id(trace_id):\n",
    "    url = f\"http://tempo:3200/api/traces/{trace_id}\"\n",
    "    \n",
    "    # Make the API call and get the JSON response\n",
    "    response = requests.get(url)\n",
    "    response_json = response.json()\n",
    "    \n",
    "    # the JSON response\n",
    "    json_data = json.dumps(response_json, indent=2)\n",
    "    data = json.loads(json_data)\n",
    "\n",
    "    # iterate through JSON to retrieve the requestID value\n",
    "    for batch in data['batches']:\n",
    "        for scope_span in batch['scopeSpans']:\n",
    "            for span in scope_span['spans']:\n",
    "                if span['name'] == \"api - start flow\":\n",
    "                    attributes = span['attributes']\n",
    "                    flow_request_id = next(item['value']['stringValue'] for item in attributes if item['key'] == 'flow.request.id')\n",
    "    return flow_request_id\n",
    "\n",
    "# Perform the HTTP GET request\n",
    "response = requests.get(url, params=params)\n",
    "response_json = response.json()\n",
    "\n",
    "# Retrieve all traceIDs\n",
    "trace_ids = [trace[\"traceID\"] for trace in response_json[\"traces\"]]\n",
    "\n",
    "# Create a list of tuples containing trace_id and request_id\n",
    "data_tuples = [(trace_id, get_request_id(trace_id)) for trace_id in trace_ids]\n",
    "\n",
    "# Create a pandas DataFrame from the list of tuples\n",
    "df = pd.DataFrame(data_tuples, columns=[\"trace ID\", \"request ID\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e625d57-8f8f-4614-a171-6b0e926842f3",
   "metadata": {},
   "source": [
    "Below is the script used to convert the data from the zipkin server to a more useful format with the hops and lag calculated, and displaying it as a Pandas dataframe spreadsheet. If anymore analysis is required, perhaps more case specific, the code can be altered and further cells may be added!\n",
    "\n",
    "Enhance the `filter()` function for refined event logging in the desired spreadsheet.\n",
    "\n",
    "Run this first in order to be able to run the function at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7b33f7f0-b5cf-4e2b-838d-e182781be038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRIPT TO CREATE FORMATTED SPREADSHEET TO ILLUSTATE TRACE SPAN DURATION, LAG AND DIFFERENT HOPS IN THE SYSTEM\n",
    "\n",
    "# Define Span class\n",
    "class Span:\n",
    "    def __init__(self):\n",
    "        self.children = []\n",
    "\n",
    "# This method aggregates a span and all of its children into a list\n",
    "def follow_chain(first_span):\n",
    "    chain = [first_span]\n",
    "    while first_span.children:\n",
    "        first_span = first_span.children[0]\n",
    "        if first_span.operation_name != \"poll\":\n",
    "            chain.append(first_span)\n",
    "        if first_span.operation_name.startswith(\"flow event\"):\n",
    "            break\n",
    "    return chain\n",
    "\n",
    "# This method filters out unwanted event span logs\n",
    "def filter(input_file):\n",
    "    \n",
    "    # Filtering send batch and send state records\n",
    "    intermediate_file = \"intermediate.csv\"\n",
    "    with open(input_file, 'r') as csv_file, open(intermediate_file, 'w', newline='') as outfile:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        fieldnames = csv_reader.fieldnames\n",
    "        csv_writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "    \n",
    "        for row in csv_reader:\n",
    "            operation_name = row['operationName']  \n",
    "    \n",
    "            if not operation_name.startswith('send batch'):\n",
    "                csv_writer.writerow(row)  # Write the row to the output file if the condition is not met\n",
    "    return intermediate_file\n",
    "    \n",
    "    \n",
    "# This method retrives all trace span data of a given trace as a JSON and formats it into a csv file\n",
    "def retrieve_trace_data(trace_id):\n",
    "    # Get trace data of desired trace (specified through command line arg)\n",
    "    url = f\"http://admin:admin@tempo:3200/api/traces/{trace_id}\"\n",
    "\n",
    "    # Make the API call and get the JSON response\n",
    "    response = requests.get(url)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # the JSON response\n",
    "    json_data = json.dumps(response_json, indent=2)\n",
    "\n",
    "    data = json.loads(json_data)\n",
    "\n",
    "    # Extract relevant information from the JSON data and convert it to a list of dictionaries\n",
    "    rows = []\n",
    "    for batch in data['batches']:\n",
    "        for scope_span in batch['scopeSpans']:\n",
    "            for span in scope_span['spans']:\n",
    "                trace_id = span['traceId']\n",
    "                span_id = span['spanId']\n",
    "                parent_span_id = span.get('parentSpanId', '')\n",
    "                name = span['name']\n",
    "                start_time = int(span['startTimeUnixNano']) / 10**6\n",
    "                end_time = int(span['endTimeUnixNano']) / 10**6     \n",
    "                duration = float(end_time - start_time) \n",
    "                attributes = span.get('attributes', [])\n",
    "                tags = json.dumps([{\"value\": attr['value'].get('stringValue'), \"key\": attr['key']} for attr in attributes])\n",
    "\n",
    "                row = {\n",
    "                    \"traceID\": trace_id,\n",
    "                    \"spanID\": span_id,\n",
    "                    \"parentSpanID\": parent_span_id,\n",
    "                    \"operationName\": name,\n",
    "                    \"startTime\": start_time,\n",
    "                    \"duration\": duration,\n",
    "                    \"tags\": tags\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    input_file = \"spans_data.csv\"\n",
    "    fieldnames = [\"traceID\", \"spanID\", \"parentSpanID\", \"operationName\", \"startTime\", \"duration\", \"tags\"]\n",
    "\n",
    "    with open(input_file, mode='w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    return input_file\n",
    "\n",
    "# This method generates the output spreadsheet\n",
    "def generate(trace_id):\n",
    "\n",
    "    # Getting raw trace data csv\n",
    "    input_file = retrieve_trace_data(trace_id)\n",
    "    \n",
    "    # Call filtering function\n",
    "    intermediate_file = filter(input_file)\n",
    "\n",
    "    # List of all span objects\n",
    "    records = []\n",
    "\n",
    "    # Dictionary that matches spanID: span object (for more usability)\n",
    "    index = {}\n",
    "\n",
    "    # Safely reading from source file, and creating span objects for all the rows (spans) in the source CSV file\n",
    "    with open(intermediate_file, 'r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            span = Span()\n",
    "            span.id = row['spanID']\n",
    "            span.parent_id = row['parentSpanID']\n",
    "            span.operation_name = row['operationName']\n",
    "            span.start_time_ms = float(row['startTime'])\n",
    "            span.start_time = datetime.datetime.utcfromtimestamp(span.start_time_ms / 1000.0)\n",
    "            span.duration = float(row['duration'])\n",
    "            span.topic = None\n",
    "            if row['tags']:\n",
    "                tags = json.loads(row['tags'])\n",
    "                span.topic = next((tag['value'] for tag in tags if tag['key'] == 'kafka.topic'), None)\n",
    "            records.append(span)\n",
    "            index[span.id] = span\n",
    "\n",
    "    # Populating each span's children field with their children if any\n",
    "    for record in records:\n",
    "        if record.id == record.parent_id or not record.parent_id:\n",
    "            continue\n",
    "        parent = index[record.parent_id]\n",
    "        parent.children.append(record)\n",
    "\n",
    "    # Retrieve all spans related to flow events and order in ascending start time\n",
    "    flow_events = [record for record in records if record.operation_name.startswith(\"flow event\")]\n",
    "    flow_events = sorted(flow_events, key=attrgetter(\"start_time_ms\"))\n",
    "\n",
    "    # Create one large list: flow_event_chains, where all the elements are the flow event chains for all flow events:\n",
    "    # For example, if the trace has 2 flow events, then the flow_event_chains will have 2 elements, each of these elements will\n",
    "    # be a tuple with the first element being the flow event span and the second element being a list containing all of its children.\n",
    "    flow_event_chains = list(chain.from_iterable(([(event, follow_chain(child)) for child in event.children]) for event in flow_events))\n",
    "\n",
    "    # Calculate absolute start time for flow sequence\n",
    "    start_time = min(event.start_time_ms for event in flow_events)\n",
    "\n",
    "    # Setting the spreadsheet headers in accrodance with the max number of hops present\n",
    "    hops = max(len(flow_event_chain[1]) for flow_event_chain in flow_event_chains)\n",
    "    title1 = ['Summary', '', '', 'Starting Event']\n",
    "    for i in range(hops):\n",
    "        if i == 0:\n",
    "            title1.extend(['', '', '', 'Hop ' + str(i + 1)])\n",
    "        else:\n",
    "            title1.extend(['','','Hop ' + str(i + 1)])\n",
    "    title1.extend(['',''])\n",
    "\n",
    "    title2 = ['Start Offset (ms)', 'Total Duration (ms)', 'Total Lag (ms)', 'Process Flow Event', 'Group', 'Duration (ms)', 'Lag (ms)']\n",
    "    title2.extend(['Span', 'Duration (ms)', 'Lag (ms)'] * hops)\n",
    "\n",
    "    # Populate a list with all the formatted spans\n",
    "    data = []\n",
    "    for index, flow_event_chain in enumerate(flow_event_chains):\n",
    "        line = []\n",
    "\n",
    "        # Flow event span\n",
    "        root_item = flow_event_chain[0]\n",
    "        line.append(round(root_item.start_time_ms - start_time, 2))\n",
    "\n",
    "        total_lag = 0.0\n",
    "        total_duration = 0.0\n",
    "\n",
    "        segment = []\n",
    "        current_item = flow_event_chain[0]\n",
    "\n",
    "        # For all the children of the root flow event span\n",
    "        for flow_event in flow_event_chain[1]:\n",
    "            flow_event.lag = flow_event.start_time_ms - current_item.start_time_ms - current_item.duration\n",
    "            segment.append(flow_event.operation_name)\n",
    "            segment.append(str(round(flow_event.duration, 2)))\n",
    "            segment.append(str(round(flow_event.lag, 2)))\n",
    "            total_lag += flow_event.lag\n",
    "            total_duration += flow_event.duration\n",
    "            current_item = flow_event\n",
    "\n",
    "        line.append(round(total_duration, 2))\n",
    "        line.append(round(total_lag, 2))\n",
    "        line.append(root_item.operation_name)\n",
    "        line.append(index)\n",
    "        line.append(round(root_item.duration, 2))\n",
    "        line.append(\"0\")  # Lag is always 0 for the first item.\n",
    "        line.extend(segment)\n",
    "\n",
    "        data.append(line)\n",
    "    \n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data, columns=title2)\n",
    "    \n",
    "    # Original column names (header row)\n",
    "    original_headers = df.columns.tolist()\n",
    "    \n",
    "    # Concatenate the extra header row with the original DataFrame columns\n",
    "    columns = pd.MultiIndex.from_arrays([title1, original_headers])\n",
    "    \n",
    "    # Set the new columns with the MultiIndex\n",
    "    df.columns = columns\n",
    "\n",
    "    # Remove all intermediate CSV files\n",
    "    os.remove(intermediate_file)\n",
    "    os.remove(input_file)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Display the DataFrame with the additional header row\n",
    "    display(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84366a23-3f51-4926-b851-e0922b23545c",
   "metadata": {},
   "source": [
    "Lastly, we can select the trace ID of the flow we are interested in and use it as input to generate our spreadsheet:\n",
    "NOTE, for this 'main()' function to work ensure you run the cell above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5bad8c21-ae59-4dd9-9366-056d2d85d119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>Starting Event</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>Hop 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>Hop 2</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Start Offset (ms)</th>\n",
       "      <th>Total Duration (ms)</th>\n",
       "      <th>Total Lag (ms)</th>\n",
       "      <th>Process Flow Event</th>\n",
       "      <th>Group</th>\n",
       "      <th>Duration (ms)</th>\n",
       "      <th>Lag (ms)</th>\n",
       "      <th>Span</th>\n",
       "      <th>Duration (ms)</th>\n",
       "      <th>Lag (ms)</th>\n",
       "      <th>Span</th>\n",
       "      <th>Duration (ms)</th>\n",
       "      <th>Lag (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>41.45</td>\n",
       "      <td>7.66</td>\n",
       "      <td>flow event - startflow</td>\n",
       "      <td>0</td>\n",
       "      <td>2028.50</td>\n",
       "      <td>0</td>\n",
       "      <td>send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d</td>\n",
       "      <td>25.98</td>\n",
       "      <td>0.24</td>\n",
       "      <td>flow event - wakeup</td>\n",
       "      <td>15.47</td>\n",
       "      <td>7.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>23.04</td>\n",
       "      <td>3.23</td>\n",
       "      <td>flow event - startflow</td>\n",
       "      <td>1</td>\n",
       "      <td>2028.50</td>\n",
       "      <td>0</td>\n",
       "      <td>send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d</td>\n",
       "      <td>23.04</td>\n",
       "      <td>3.23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2062.13</td>\n",
       "      <td>12.66</td>\n",
       "      <td>0.73</td>\n",
       "      <td>flow event - wakeup</td>\n",
       "      <td>2</td>\n",
       "      <td>15.47</td>\n",
       "      <td>0</td>\n",
       "      <td>send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d</td>\n",
       "      <td>12.66</td>\n",
       "      <td>0.73</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2062.13</td>\n",
       "      <td>12.47</td>\n",
       "      <td>6.42</td>\n",
       "      <td>flow event - wakeup</td>\n",
       "      <td>3</td>\n",
       "      <td>15.47</td>\n",
       "      <td>0</td>\n",
       "      <td>send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d</td>\n",
       "      <td>12.43</td>\n",
       "      <td>0.87</td>\n",
       "      <td>flow mapper event - schedulecleanup</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Summary                                             Starting Event                                                                                                                   Hop 1                                                       Hop 2                       \n",
       "  Start Offset (ms) Total Duration (ms) Total Lag (ms)      Process Flow Event Group Duration (ms) Lag (ms)                                                                                       Span Duration (ms) Lag (ms)                                 Span Duration (ms) Lag (ms)\n",
       "0              0.00               41.45           7.66  flow event - startflow     0       2028.50        0  send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d         25.98     0.24                  flow event - wakeup         15.47     7.42\n",
       "1              0.00               23.04           3.23  flow event - startflow     1       2028.50        0  send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d         23.04     3.23                                 None          None     None\n",
       "2           2062.13               12.66           0.73     flow event - wakeup     2         15.47        0  send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d         12.66     0.73                                 None          None     None\n",
       "3           2062.13               12.47           6.42     flow event - wakeup     3         15.47        0  send state_and_event--floweventconsumer--flow.event--3c32e9de-87d7-4b07-a20d-8338149be36d         12.43     0.87  flow mapper event - schedulecleanup          0.04     5.55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate spreadsheet\n",
    "generate(\"64de181b901a77350e05bdb5e114eddb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4aac2dfe-9930-48d6-a621-6289ebeebcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---  code for the formatting of the excel file --- ###\n",
    "\n",
    "    # # Define colours\n",
    "    # GREEN = '#c4e4b4'\n",
    "    # YELLOW = '#fcf4cc'\n",
    "    # BLUE = '#b4c4e4'\n",
    "\n",
    "    # # Create an Excel writer using XlsxWriter as the engine\n",
    "    # output_file = 'output.xlsx'\n",
    "    # writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "\n",
    "    # # Write the DataFrame to the Excel file\n",
    "    # df.to_excel(writer, sheet_name='Sheet1', startrow=1, index=False)\n",
    "\n",
    "    # # Get the xlsxwriter workbook and worksheet objects\n",
    "    # workbook = writer.book\n",
    "    # worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # # Add some cell formats for title1 and colour of cells\n",
    "    # format1 = workbook.add_format({'bold': True, 'border': 1})\n",
    "    # format_light_blue = workbook.add_format({'bg_color':BLUE})\n",
    "    # format_green = workbook.add_format({'bg_color':GREEN})\n",
    "    # format_line = workbook.add_format().set_right(1)\n",
    "    # format_yellow = workbook.add_format({'bg_color': YELLOW})\n",
    "\n",
    "    # # Write title1 to the Excel file as the first row with formatting\n",
    "    # for col_num, value in enumerate(title1):\n",
    "    #     worksheet.write(0, col_num, value, format1)\n",
    "\n",
    "    # # Write title2 to the Excel file as the second row with formatting\n",
    "    # for col_num, value in enumerate(title2):\n",
    "    #     # Apply appropriate colour formatting depending on cell classification\n",
    "    #     if 'Duration' in value:\n",
    "    #         worksheet.write(1, col_num, value,workbook.add_format({'bg_color': GREEN, 'bold': True, 'border': 1}))\n",
    "    #     elif 'Lag' in value:\n",
    "    #         worksheet.write(1, col_num, value,workbook.add_format({'bg_color': BLUE, 'bold': True, 'border': 1}))\n",
    "    #     elif 'Span' in value:\n",
    "    #         worksheet.write(1, col_num, value,workbook.add_format({'bg_color': YELLOW, 'bold': True, 'border': 1}))\n",
    "    #     else:\n",
    "    #         worksheet.write(1, col_num, value, format1)\n",
    "\n",
    "\n",
    "    # # Autofit the column width to fit the content\n",
    "    # for i, col in enumerate(title2):\n",
    "    #     max_len = 0\n",
    "    #     for item in df.iloc[:, i].items():\n",
    "    #         if len(str(item[1])) > max_len:\n",
    "    #             max_len = len(str(item[1]))\n",
    "    #     max_len = max(max_len, len(col))\n",
    "    #     column_width = max_len + 2  # Add some extra space for padding\n",
    "    #     worksheet.set_column(i, i, column_width)\n",
    "\n",
    "    #     # Apply appropriate colour formatting depending on cell classification\n",
    "    #     if 'Duration' in col:\n",
    "    #         worksheet.set_column(i, i, column_width, format_green)\n",
    "    #     elif 'Lag' in col:\n",
    "    #         worksheet.set_column(i, i, column_width, format_light_blue)\n",
    "    #     elif 'Span' in col:\n",
    "    #         worksheet.set_column(i, i, column_width, format_yellow)\n",
    "\n",
    "\n",
    "    # # Close the Pandas Excel writer and output the Excel file\n",
    "    # writer._save()\n",
    "\n",
    "    # '''\n",
    "    # The code block below is a VERY SCRAPPY AND BAD WAY to Add black border separations between spans\n",
    "    # '''\n",
    "    # wb_style = openpyxl.load_workbook(output_file)\n",
    "\n",
    "    # # Accessing Product Information Sheet\n",
    "    # sheet = wb_style.active\n",
    "    # thin = Side(border_style=\"thin\", color=\"000000\")\n",
    "\n",
    "    # blue_fill = PatternFill(start_color='b4c4e4', end_color='b4c4e4', fill_type='solid')\n",
    "\n",
    "    # # Iterate through all cell values in the second row\n",
    "    # for col_idx, cell in enumerate(sheet[2], 1):\n",
    "    #     if 'Lag' in cell.value:\n",
    "    #         for cell in sheet.iter_rows(min_row=3, max_row=sheet.max_row, min_col=col_idx, max_col=col_idx):\n",
    "    #             for row_cell in cell:\n",
    "    #                 row_cell.border = Border(top=None, left=None, right=thin, bottom=None)\n",
    "    #                 row_cell.fill = blue_fill\n",
    "\n",
    "    # # Saving the modified workbook\n",
    "    # wb_style.save(output_file)\n",
    "\n",
    "    # e_df = pd.read_excel(output_file)\n",
    "    # display(e_df)\n",
    "    # # Delete intermediate files used when filtering the send batch records\n",
    "    # os.remove(intermediate_file)\n",
    "    # os.remove(input_file)\n",
    "\n",
    "    # return output_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
